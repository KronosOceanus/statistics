[TOC]

*** 正交 $u u^T = 0​$ ***

# 极大似然估计

## 极大似然方法

### 极大似然估计

#### 高斯模型（多维概率分布）

#### 主成分分析

* 数据 $X = (x_1,...,x_n)$，其中 $x_i$ 是向量，描述事物特征，$n$ 是样本数
* 变换 $y_i = u^Tx_i$，其中 $y_i$ 是主成分，$u^T$ 是载荷因子（特征向量）
* 使数据点距离回归线距离方差最小：$max_{u} \sum_{i=1}^n\frac{1}{n}(u^Tx_i-u^T\overline{x})^2​$ $s.t​$  $ u^Tu = 1​$（$s.t​$ 后面表示约束条件）
* 继续：$= max_u [u^TSu+ \lambda(1-u^Tu)]$ ，其中 $S = \frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^T(x_i-\overline{x})$
* 似然函数：$L = u^TSu + \lambda(1-u^Tu)$，对 $u$ 求导
* 得到：$Su = \lambda u$，$S$ 是矩阵，该式子目的是计算最大特征值 $\lambda$ 

###### 计算最大特征值（补充）

* 如上式：计算行列式 $|\lambda E-S| = 0$ 的根即为特征值（特征值数与变量维数相等）
* 根 $\lambda_i$ 对应的特征向量满足 $(\lambda_iE-S)u_i = 0$，计算得特征向量 $u_i$

##### 例：PCA 鸢尾花数据降维

* 模型：$X = (x_1,...x_n)$ ，其中 $x_i$ 为 4 维向量，$n = 150$
* 计算协方差矩阵：$S = \frac{1}{n}\sum_{i=1}^n(x_i-\overline{x})^T(x_i-\overline{x})$ ，$4\times 4$ 矩阵（$\sum$ 为矩阵加法）
* 计算 $S$ 的 2（降维之后的维数）个最大特征值对应的特征向量：$u_1$ , $u_2$ 
* 由 $y_i = u_i^Tx$ 得到 2 个主成分，画出二维平面图

### 类后验计算

* 用于数据分类

* 贝叶斯公式：$p(y|X) = \frac{p(X|y)p(y)}{p(X)}$
* 后验估计：$ln p(y|X) = lnp(X|y)+lnp(y)-lnp(X)$
* 继续（将 $p(X|y)$ 看作高斯分布）：$= -\frac{d}{2}ln(2\pi)-\frac{1}{2}ln(det(\sum_y))-\frac{1}{2}(x-\mu_y)^T\sum_y^{-1}(x-\mu_y) + ln\frac{n_y}{n}-lnp(X)$（其中 $\mu_y$ 是 $y$ 的均值，也是向量）
* 继续：$= -\frac{1}{2}ln(det(\sum_y))-\frac{1}{2}(x-\mu_y)^T\sum_y^{-1}(x-\mu_y) + ln\frac{n_y}{n} +C$（其中 $\frac{n_y}{n}$ 是频率）

#### Fisher-LDA

* 将高维样本通过数学变换投影到一维空间中再对其进行分类

* 模型：假设上面数据协方差都是 $$\sum=\frac{1}{n}\sum_{y=1}^c\sum_{i:y_i=y}(x_i-\mu_y)^T(x_i-\mu_y)=\sum_{y=1}^c\frac{n_y}{n}\sum_y$$（其中 $c$ 为分类数），则 $$lnp(y|X) =  x^T\sum^{-1}\mu_y-\frac{1}{2}\mu_y^T\sum^{-1}\mu_y +  ln\frac{n_y}{n} +C'$$（将 $\sum_y$ 化成 $\sum$）
* 假设分两类，两类作差：$lnp(y_1|X)-lnp(y_2|X) = X^T\sum^{-1}(\mu_1-\mu_2)-\frac{1}{2}(\mu_1^{T}\sum^{-1}\mu_1-\mu_2^{T}\sum^{-1}\mu_2) + ln\frac{n_1}{n_2}$
* 分类边界：$aX + b = 0$，即 $a = \sum^{-1}(\mu_1-\mu_2) $，$b = -\frac{1}{2}(\mu_1^{T}\sum^{-1}\mu_1-\mu_2^{T}\sum^{-1}\mu_2) + ln\frac{n_1}{n_2}$

#### Rank reduced-LDA

* 模型：假设两类样本 $y_1$~ $N(\mu_1,\sum)$， $y_2$~ $N(\mu_2,\sum)$
* 找到直线 $w$ ，使 $||w^T\mu_1w^T\mu_2||_2^2$ 大，$w^T\sum w$ 小
* 定义：类内散度 $S_{w} = (\mu_1-\mu_2)(\mu_1-\mu_2)^T$（小），类间散度 $\sum = S_b = \frac{n_1}{n}\sum_1+\frac{n_2}{n}\sum_2$（大）
* 即求：$L = max_{w}\frac{w^TS_w w}{w ^TS_bw}$
* 令：$y=w^TS_b^{\frac{1}{2}}$
* 则：$L = max_y\frac{w^TS_b^{\frac{1}{2}}S_b^{-\frac{1}{2}}S_wS_b^{-\frac{1}{2}}S_b^{\frac{1}{2}}w}{yy^T}$
* 令：$S = S_b^{-\frac{1}{2}}S_wS_b^{-\frac{1}{2}}$，$yy^T = C（常数）
* 则：$L= max_y\frac{ySy^T}{yy^T}$，即 $max_yySy^T s.t$  $yy_T = C$
* 则：$L' = ySy^T+\lambda(C-yy^T)$，变成了 PCA 问题

*** $||...||$ 用于估计值之间的运算 ***

## 极大似然估计性质

### 一致性

* $\theta^*$ 为理论最优解，当样本数趋近无穷时
* $lim_{n\to \infty} P(||\hat{\theta}-\theta^*|| \geq \epsilon) = 0$（用大数定理可证明）

### 渐进无偏性

* 即 $n\to\infty$ 时 ，$E(\hat{\theta}) = \theta^*$  

##### 例：

* 验证 $\hat{\theta}$ 是 $y=X\theta^* + \eta$ 是无偏估计
* 解：$\hat{\theta} = (X^TX)^{-1}X^Ty = (X^TX)^{-1}X^T(X\theta^*+\eta) = I\theta^* + E(\eta) = \theta^*$，（其中 $I$ 是单位矩阵，$\eta$ 为常数，$E(\eta)=0$）

#### 方差分解定理

* 定义：最小均方误差 $E(||\hat{\theta}-\theta^*||^2)$，方差 $E(||\hat{\theta}-E(\hat{\theta})||^2) $，偏差 $||E(\hat{\theta})-\theta^*||^2$
* 则：$E(||\hat{\theta}-\theta^*||^2) = E(||\hat{\theta}-E(\hat{\theta})||^2) +||E(\hat{\theta})-\theta^*||^2$

### 渐进有效性

* 对于一个估计，$n\to\infty$ 时，如果方差能取到下界，则有效

#### Cramer-Rao 不等式

* 任何无偏估计的方差的下界：$Var(\hat\theta) \geq \frac{1}{nF(\theta^*)}$
* 其中 n 为数据的维数，$\hat\theta$ 为无偏估计值，$F(\theta^*) = \frac{\partial^2lnP(x;\theta)}{\partial\theta^2}$（Fisher 信息量） 

###### 推导：

* 为了使 $E(\hat\theta)$ = $E(\theta)$ ，即求 $E(\hat\theta-\theta) = 0$，（其中 $\theta$ 为真实值，所以 $E(\theta) = \theta$）

* 求偏导：$\frac{\partial}{\partial\theta}[\int(\hat\theta-\theta)P(x;\theta)] = \int\frac{\partial}{\partial\theta}[(\hat\theta-\theta)P(x;\theta)]dx = 0$
* 得：$1 = [\int(\hat\theta-\theta)\sqrt{P(x;\theta)}\sqrt{P(x;\theta)}\frac{\partial lnP(x;\theta)}{\partial\theta}dx]^2$
* 根据柯西不等式（$|E(X, Y)| \leq [E(X^2)E(Y^2)]^{\frac{1}{2}}$）得：$$1 \leq \int(\hat\theta-\theta)^2P(x;\theta)dx\int P(x;\theta)[\frac{\partial }{\partial\theta}lnP(x;\theta)]^2dx$$（两部分）
* 即：$1 \leq Var(\hat\theta)\times E([\frac{\partial }{\partial\theta}lnP(x;\theta)]^2)$，（其中 $E([\frac{\partial }{\partial\theta}lnP(x;\theta)]^2) = F(\theta^*)$ / 即 Fisher 信息量）

###### 小技巧（接下来推导会用到）

* $\frac{\partial ln f}{\partial g}= \frac{1}{f}\frac{\partial f}{\partial g}$

##### Fisher 信息量

* 描述了在给定已知的 $\theta$ 值的情况下观察给定样本 $x$ 的概率
* 多维矩阵的 Fisher 信息量：$$F(\theta) = E([\frac{\partial }{\partial\theta}lnP(x;\theta)]^2)=\int(\frac{\partial}{\partial\theta}lnP(x;\theta))(\frac{\partial}{\partial\theta^T}lnP(x;\theta))P(x;\theta)dx​$$（关于 $\theta​$ 一阶导数平方的期望）
* 等价于：$F(\theta) = -\int(\frac{\partial^2}{\partial\theta\partial\theta^T}lnP(x;\theta))P(x;\theta)dx$（关于 $\theta$ 二阶导数的期望）

###### 证明上式

* $$F(\theta) = -\int(\frac{\partial^2}{\partial\theta\partial\theta^T}lnP(x;\theta))P(x;\theta)dx= \int\frac{\partial}{\partial\theta}[\frac{\partial}{\partial\theta^T}lnP(x;\theta)]$$

* 证明：$$=\int(\frac{\partial^2}{\partial\theta\partial\theta^T}lnP(x;\theta))P(x;\theta)dx= \int\frac{\partial}{\partial\theta}[\frac{\partial}{\partial\theta^T}lnP(x;\theta)]P(x;\theta)dx$$
* 继续：$=\frac{\partial}{\partial\theta}[\frac{1}{P(x;\theta)}\frac{\partial}{\partial\theta^T}P(x;\theta)]$
* 复合函数求导：$$=\int\frac{\frac{\partial^2}{\partial\theta\partial\theta^T}P(x;\theta)}{P(x;\theta)}P(x;\theta)dx-\int\frac{\frac{\partial}{\partial\theta} P(x;\theta)\frac{\partial}{\partial\theta^T} P(x;\theta)}{P(x;\theta)^2}P(x;\theta)$$
* 继续：$$=\int\frac{\partial^2}{\partial\theta\partial\theta^T}P(x;\theta)dx-\int(\frac{\partial}{\partial\theta}lnP(x;\theta))(\frac{\partial}{\partial\theta^T}lnP(x;\theta))P(x;\theta)dx$$
* 继续：$$=\frac{\partial^2}{\partial\theta\partial\theta^T}\int P(x;\theta)dx-F(\theta)=\frac{\partial^2}{\partial\theta\partial\theta^T}1-F(\theta)=-F(\theta)$$

### 渐进正态性

* 当样本 $n\to\infty$，估计量服从正态分布
* 极大似然估计：$\xi(\theta) = \frac{\partial}{\partial\theta}\frac{1}{n}\sum_{i=1}^nlnP(x_i;\theta)$
* 泰勒级数（在 $\theta^*$ 处）展开：$\xi(\theta)=\xi(\theta^*)+\xi'(\theta^*)(\theta-\theta^*)+\frac{\xi''(\theta^*)}{2!}(\theta-\theta^*)^2+...$
* 得到：$\xi(\theta^*)-F(\theta^*)(\theta_{ML}-\theta^*)+r=0_b$
* 其中：$\xi(\theta^*)$ 为零阶项，（取极值，一阶项为 0），$F(\theta^*)(\theta-\theta^*)$ 为二阶项，$r$ 为高阶项，$F(\theta)=-\frac{1}{n}\sum_{i=1}^n\frac{\partial^2}{\partial\theta\partial\theta^T}lnP(x;\theta)$
* 省略高阶项：$(\theta-\theta^*)=\frac{\xi(\theta^*)}{F(\theta^*)}$
* 同乘 $\sqrt{n}$ 得：$\sqrt{n}(\theta-\theta^*)F(\theta^*)=\sqrt{n}\xi(\theta^*)$ ~ $N(0_b,F(\theta^*))$
* 其中因为 $\theta^*$ 是对最大化参数的 $E(lnP(x;\theta))$，所以 $E(\xi(\theta) = 0)$ （极大似然函数 = 0），$Var(\xi(\theta))= F(\theta^*)$
* 最后根据中心极限定理（$\frac{n\xi(\theta)-E(\xi(\theta))}{\sqrt{nVar(\xi(\theta))}}$ ~ $N(0_b, I)$）得：$\sqrt{n}\xi(\theta^*)$ ~ $N(0_b,F(\theta^*))$

## 模型选择

* 超参数：事先给定的模型的参数
* 参数数量叫做模型复杂性，样本数量叫做样本复杂性，选择参数将这两个参数匹配

### AIC 

#### Kullback-Leibler-散度

* 两个概率分布间的“距离”
* $KL(P||Q)=E_p(ln\frac{P(x)}{Q(x)})=E_p(lnP(x))-E_p(lnQ(x)) \geq 0$
* 熵（概率分布的随机性）：$H(x) = -E_p(lnP(x))$
* 互信息：$I(x,y)=KL(P(x,y)||P(x)P(y))$，（$P(x,y)=P(x)P(y)$独立时，$I(x,y)=0$）

##### 计算两个高斯分布的 KL-散度

* 两个分布：$P(x) = N(x_i;\mu,\sum)$，$Q(x) = N(x_i;m,L)$
* 假设：$x​$ ~ $N(\mu,\sum)​$，则 $E_p(x) = \int xP(x)dx=\mu​$（以下 $E_P​$ 用 $E​$ 代替）
* 应用到的矩阵微积分：$E((x-a)^TA(x-a))=tr(A\sum)+(\mu-a)^TA(\mu-a)$
* 计算：$$KL=\int[\frac{1}{2}ln\frac{|L|}{|\sum|}-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)+\frac{1}{2}(x-m)^TL^{-1}(x-m)]P(x)dx$$
* 继续：$=\frac{1}{2}ln\frac{|L|}{|\sum|}-\frac{1}{2}E[(x-\mu)^T\sum^{-1}(x-\mu)]+\frac{1}{2}E[(x-m)^TL^{-1}(x-m)]$
* 继续（矩阵微积分）：$=\frac{1}{2}ln\frac{|L|}{|\sum|}-\frac{1}{2}tr(I_D)+\frac{1}{2}tr(L^{-1}\sum)+\frac{1}{2}(\mu-m)^TL^{-1}(\mu-m)$
* 继续（提出 $\frac{1}{2}$）：$=\frac{1}{2}[ln\frac{|L|}{|\sum|}-tr(I_D)+tr(L^{-1}\sum)+(\mu-m)^TL^{-1}(\mu-m)]$

##### 上式矩阵微积分证明

###### 引理1

* 对于向量$\lambda$，公式 $\lambda^TA\lambda$ 的结果是一个标量，即：$\lambda^TA\lambda = tr(\lambda^TA\lambda) = tr(A\lambda^T\lambda)$

###### 引理2

* 期望与协方差的性质：$E(x^Tx)=\sum + \mu^T\mu$
* 按照 $\sum=E((x-\mu)(x-\mu)^T)$ 证明

###### 引理3

* $E(x^TAx)=tr(A\sum)+\mu^TA\mu$

* 证明：$$E(x^TAx)=E(tr(x^TAx))=E(tr(Ax^Tx))=tr(E(Ax^Tx))=tr(AE(x^Tx))$$
* 继续：$$=tr(A(\sum+\mu^T\mu))=tr(A\sum)+tr(A\mu^T\mu)=tr(A\sum)+\mu^TA\mu$$

###### 证明

* $E((x-a)^TA(x-a))=E(x^TAx)-2E(xAa)+E(a^TAa)$
* 继续（应用上面引理）：$=tr(A\sum)+\mu^TA\mu-2aA\mu+a^TAa=tr(A\sum)+(\mu-a)^TA(\mu-a)$

#### 引理

* 负对数似然函数的期望：$J(\theta) = -E(lnP(x;\theta))$
* 令 $\theta^*=argmin_{\theta}J(\theta)$ 得：$$E(J)=-E[\frac{1}{n}\sum_{i=1}^nlnP(x_i;\theta)]+\frac{1}{n}tr[Q(\theta^*)G(\theta^*)^{-1}]+o(n^{-1})$$（ $n$ 的高阶无穷小，$n$ 表示样本数量）

#### 原则

* 其中：$Q(\theta) = E(\frac{\partial}{\partial\theta}lnP(x;\theta)\frac{\partial}{\partial\theta^T}lnP(x;\theta))$（一阶导平方的期望），$G(\theta) = -E(\frac{\partial^2}{\partial\theta\partial\theta^T}lnP(x;\theta))$（二阶导的期望）
* Fisher 信息量证明：$Q(\theta) = -G(\theta)$
* 得：$tr(Q(\theta^*)G(\theta^*)^{-1})= tr(I_b) = b$（其中 b 是模型参数个数）
* 即：$AIC = -lnL+b$（其中 L 为似然函数/尽可能小，b 为模型参数个数/尽可能少）

### 交叉验证

* 目的：选取超参数，减小模型过拟合

* 模型：将数据 $D=(x_1,...,x_n)$ 分成大小基本相同的划分 $D_l,...,D_t$
* 对于每个划分 $D_l$，使用 $D_l$ 之外的数据训练模型，用 $D_l$ 测试模型计算出对数似然 $J_l$
* 取最大平均得分 $\frac{1}{t}\sum_{l=1}^tJ_l$ 时的参数值



