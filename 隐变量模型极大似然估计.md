[TOC]

# 隐变量模型的极大似然估计

#### SoftMax 函数与归一化

* 将 $X=(x_1,...,x_n)$ 变量用函数 $y_i=\frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}$ 投影到 $Y=(0,1)$ 区间内，$\sum_{i=1}^ny_i=1$ 

## 隐变量图模型

* 隐变量：参数向量中的某个参数值
* （隐变量）期望最大化算法

### 高斯混合模型

* 模型：加权平均高斯模型 $P(x;\theta)=\sum_{l=1}^nw_lN(x;\mu,\sum)$
* $N(x;\mu,\sum)=\frac{det(\sum^{-\frac{1}{2}})}{(2\pi)^{\frac{d}{2}}}e^{-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)}$
* 参数：$\theta = (w_1,...,w_m, \mu_1,...,\mu_m, \sum_1,...,\sum_m)$
*  $\int P(x;\theta)dx=1\to\sum_{l=1}^m...=1$

#### 重参数化

* 利用 SoftMax 函数将隐变量参数化：$w_l=\frac{e^{\gamma_l}}{\sum_{k=1}^me^{\gamma_k}}$ （将之前的 $w_l$ 看作 $\gamma_l$，得到新的 $w_l$）
* 极大似然估计：$ln L(\theta) = \sum_{i=1}^nln\sum_{k=1}^me^{\gamma_k}N(x_i;\mu_k,\sum_k)-nln\sum_{k=1}^me^{\gamma_k}$（后面是拉格朗日乘子） 
* 求偏导与多元高斯分布类似，区别是多元高斯分布对向量求偏导
* 对 $\gamma_l$ 求偏导（ $\gamma_l$ 表示其中的一个权）：$$\frac{\partial lnL(\theta)}{\partial\gamma_l}=\sum_{i=1}^n\frac{e^{\gamma_l}N(x_i;\mu_l,\sum_l)}{\sum_{k=1}^me^{\gamma_k}N(x_i;\mu_k,\sum_k)}-n\frac{e^{\gamma_l}}{\sum_{k=1}^me^{\gamma_k}}=\sum_{i=1}^n\eta_{il}-nw_l=0$$
* 对 $\mu_l$ 求偏导：$$\frac{\partial lnL(\theta)}{\partial\mu_l}=\sum_{i=1}^n\frac{e^{\gamma_l}N(x_i;\mu_l,\sum_l)}{\sum_{k=1}^me^{\gamma_k}N(x_i;\mu_k,\sum_k)}\sum_l^{-1}(x_i-\mu_l)=\sum_{i=1}^n\eta_{il}\sum_l^{-1}(x_i-\mu_l)=0$$
* 对 $\sum_l$ 求偏导：$$\frac{\partial lnL(\theta)}{\partial\sum_l}=\frac{1}{2}\sum_{i=1}^n\eta_{il}(\sum_l^{-1}(x_i-\mu_l)(x_i-\mu_l)^T\sum_l^{-1}-\sum_l^{-1})=0$$

#### EM 算法

* 求偏导与更新 $\eta_{il}$ 交替进行
* 给定 $\eta_{il}$

## EM 算法及证明

目的：不断更新参数 $\theta$，修正分布函数

#### 使用条件

* 直接计算 $P(X|\theta)$  的似然函数不好计算
* 引入隐变量 $Z$：$$P(X|\theta)=\frac{P(X,Z|\theta)}{P(Z|\theta)}$$

#### 求下界

* ln 似然函数：$$L(\theta|X) = lnP(X|\theta)$$
* 继续（假设 $Z​$ 的分布是 $Q(Z)​$）： $$=ln(\frac{P(X,Z|\theta)}{P(Z|\theta)})=ln(\frac{\frac{P(X,Z|\theta)}{Q(Z)}}{\frac{P(Z,\theta)}{Q(Z)}})=ln(\frac{P(X,Z|\theta)}{Q(Z)})+ln(\frac{Q(Z)}{P(Z|\theta)})​$$
* 两边对 $Z$ 求期望：$$lnP(X|\theta) = \int_Zln(\frac{P(X,Z|\theta)}{Q(Z)})Q(Z)+\int_Zln(\frac{Q(Z)}{P(Z|\theta)})Q(Z)$$
* 继续：$= \int_Zln(\frac{P(X,Z|\theta)}{Q(Z)})Q(Z)+KL(Q(Z)||P(Z|\theta))$（KL-散度$KL(P||Q)=E_p(ln\frac{P(x)}{Q(x)}) \geq 0$）
* 所以下界：$F(\theta,Q) =  \int_Zln(\frac{P(X,Z|\theta)}{Q(Z)})Q(Z)$

#### E- 步

* 固定参数 $\Theta$ ，估计 $Z$ 的分布：$Q(Z) = P(Z|\Theta_g)$（其中 $\Theta_g$ 是 EM 算法中上一步计算出的 $\theta$）

#### M- 步

* 求 $L(\Theta_g|X)$ 的极大值：$lnP(X|\theta)=\int_ZlnP(X,Z|\theta)P(Z|\Theta_g)dz-\int_ZlnP(Z|\Theta_g)P(Z|\Theta_g)dz=Q(\theta,\Theta_g)-H(\theta,\Theta_g)$
* 得（每次都选取使 $Q(\theta,\Theta_g)$ 最大的 $\theta$，所以 $Q(\Theta_{g+1},\Theta_g)\geq Q(\Theta_g,\Theta_g)$）：$argmax_{\theta}[\int_ZlnP(X,Z|\theta)P(Z|\Theta_g)dz]=\Theta_{g+1} \to H(\Theta_{g+1},\Theta_g)\leq H(\Theta_g,\Theta_g)$（辅助函数/用 $H(\Theta_g,\Theta_g)-H(\theta,\Theta_g) \geq 0$ 证明/利用 Jensen 不等式）

#### 收敛性

* $$L(\Theta_{g+1})=Q(\Theta_{g+1},\Theta_g)-H(\Theta_{g+1},\Theta_g)\geq Q(\Theta_g,\Theta_g)-H(\Theta_g,\Theta_g) = L(\Theta_g)​$$